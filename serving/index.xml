<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Serving on</title><link>https://kubedl.io/serving/</link><description>Recent content in Serving on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 06 Oct 2020 08:48:23 +0000</lastBuildDate><atom:link href="https://kubedl.io/serving/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction</title><link>https://kubedl.io/serving/intro/</link><pubDate>Thu, 12 Nov 2020 15:22:20 +0100</pubDate><guid>https://kubedl.io/serving/intro/</guid><description>KubeDL Serving provides a group of user freindly APIs to construct online model inference services. It closely cooperates with training and model stages, making end-to-end deep learning development automatically.
KubeDL provides CRD Inference to accomplish this:
Inference With Single Model Inference describes an expeced inference service including adopted framework, predictor templates, autoscaling polices&amp;hellip; An example YAML looks like below, this example shows how inference service serves single model:
apiVersion: serving.kubedl.io/v1alpha1 kind: Inference metadata: name: hello-inference spec: framework: TFServing predictors: - name: model-predictor modelVersion: model replicas: 3 autoScale: minReplicas: 1 maxReplicas: 10 batching: batchSize: 32 template: spec: containers: - name: tensorflow args: - --port=9000 - --rest_api_port=8500 - --model_name=mnist - --model_base_path=/kubedl-model/ command: - /usr/bin/tensorflow_model_server image: tensorflow/serving:1.</description></item><item><title>Design</title><link>https://kubedl.io/serving/design/</link><pubDate>Mon, 14 Jun 2021 15:22:20 +0100</pubDate><guid>https://kubedl.io/serving/design/</guid><description>This diagram illustrates the workflow from model generation to inference service deployment.
Workflow An inference CRD describes the inference framework used for serving the model (TensorRT, Triton etc.), predictor(s) template with its served model-version and deployment strategies, such as auto-scaling, batching&amp;hellip;
The Inference Controller watches the Inference CRD and does the following steps.
Retrieve the latest Inference CRD and creates an entry in cluster dns host for accessing inference.</description></item></channel></rss>